{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qnHDQ9DWSL_8"
   },
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torchvision as thv\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_pyZf0ZRSgf9",
    "outputId": "628a24b3-677a-4212-a988-71b2f2849123"
   },
   "outputs": [],
   "source": [
    "# # Use this block if you are working on Google Colab. Copies the dataset from drive. You should specify the right path\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "# !cp /content/drive/MyDrive/HW3_Data.rar HW.rar\n",
    "# !unrar x HW.rar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wFmotKK8btLu",
    "outputId": "7f7a7fbe-a1bc-47aa-9bc6-aa272f0d0835"
   },
   "outputs": [],
   "source": [
    "# Creates directories to save statistics\n",
    "!mkdir '/content/saved_losses/'\n",
    "!mkdir '/content/saved_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZRlUF_dtgjpu"
   },
   "outputs": [],
   "source": [
    "# We center the images data with the mean we found in the other script\n",
    "\n",
    "transforms_center = transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.4831, 0.4053, 0.3707),(1.0,1.0,1.0))\n",
    "])\n",
    "\n",
    "# Load datasets for train, validation and test\n",
    "dataset_centered = datasets.ImageFolder('HW3_Data/HW4_Data/classification_data/train_data',transform=transforms_center)\n",
    "dataset_validation = datasets.ImageFolder('HW3_Data/HW4_Data/classification_data/val_data',transform=transforms_center)\n",
    "dataset_test = datasets.ImageFolder('HW3_Data/HW4_Data/classification_data/test_data',transform=transforms_center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78lBMpNtbwpw"
   },
   "outputs": [],
   "source": [
    "# Implementation of the given classfier model\n",
    "\n",
    "class FaceIdentifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FaceIdentifier, self).__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 16, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 16, 5, padding=1),\n",
    "            nn.Dropout2d(),\n",
    "            nn.Conv2d(16, 16, 5),\n",
    "            nn.BatchNorm2d(16)\n",
    "        )\n",
    "\n",
    "        self.fc_block = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(6400,4000),\n",
    "            #nn.Softmax() #Softmax is implemented in the cross-entropy loss, so no need to put it here\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        logits = self.fc_block(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m-3vGvzaiEAS"
   },
   "outputs": [],
   "source": [
    "# Validate/Test Model.  Also can be renamed to evaluate.\n",
    "def validate(model, dataset, batch_size, device='cuda'):\n",
    "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "  dataset_size = len(dataloader.dataset)\n",
    "  num_batches = len(dataloader)\n",
    "\n",
    "  loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "  running_loss = 0\n",
    "  running_correct = 0\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  with th.no_grad():\n",
    "\n",
    "    for batch, (images, labels) in enumerate(dataloader):\n",
    "\n",
    "        labels = labels.to(device)\n",
    "        predictions = model(images.to(device))\n",
    "        loss = loss_fn(predictions, labels)\n",
    "\n",
    "        _, predicted_labels = predictions.max(1)\n",
    "        running_correct += predicted_labels.eq(labels).sum().item()\n",
    "        running_loss += loss.item()\n",
    "  \n",
    "  model.train()\n",
    "\n",
    "  val_loss = running_loss/num_batches\n",
    "  accuracy = running_correct/dataset_size\n",
    "\n",
    "  print(f\"VALIDATION: [loss: {val_loss:>7f}, accuracy: {accuracy}\")\n",
    "\n",
    "  return val_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2cQHA3Q6suK"
   },
   "outputs": [],
   "source": [
    "# Train the model with given parameters. Run tests on validation dataset at the end of every epoch once\n",
    "\n",
    "def train_loop(model, dataset, validation_dataset, optimizer_name='SGD', weight_decay=0, num_epochs=50, batch_size=64, device='cuda', learning_rate=1e-5, log=True):\n",
    "\n",
    "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "  loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "  epoch_losses = [] # For plotting\n",
    "  epoch_accuracies = []\n",
    "  val_losses = []\n",
    "  val_accuracies = []\n",
    "\n",
    "  if optimizer_name == 'SGD':\n",
    "    optimizer = th.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "  elif optimizer_name == 'Adam':\n",
    "    optimizer = th.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9,0.999))\n",
    "  elif optimizer_name == 'RMSProp':\n",
    "    optimizer = th.optim.RMSprop(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay)\n",
    "  elif optimizer_name == 'AdaGrad':\n",
    "    optimizer = th.optim.Adagrad(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "  dataset_size = len(dataloader.dataset)\n",
    "  num_batches = len(dataloader)\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    for batch, (images, labels) in enumerate(dataloader):\n",
    "      labels = labels.to(device)\n",
    "      predictions = model(images.to(device))\n",
    "      loss = loss_fn(predictions, labels)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      _, predicted_labels = predictions.max(1)\n",
    "      correct += predicted_labels.eq(labels).sum().item()\n",
    "      epoch_loss += loss.item()\n",
    "\n",
    "      if batch % 10 == 0 and log == True:\n",
    "        loss, current = loss.item(), batch * len(images)\n",
    "        correct_guesses = predicted_labels.eq(labels).sum().item()\n",
    "        print(f\"[Batch:{batch} Epoch:{epoch}] loss: {loss:>7f} accuracy: {correct_guesses/len(images)}  [{current:>5d}/{dataset_size:>5d}]\")\n",
    "\n",
    "    val_l, val_acc = validate(model, validation_dataset, batch_size, device)\n",
    "\n",
    "    epoch_losses.append(epoch_loss/num_batches)\n",
    "    epoch_accuracies.append(correct/dataset_size)\n",
    "    val_losses.append(val_l)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "      \n",
    "  \n",
    "  return epoch_losses, epoch_accuracies, val_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XflA3KGSYYkh",
    "outputId": "2cfdc89b-0e5d-4bde-fd65-01b0060f9314"
   },
   "outputs": [],
   "source": [
    "# Train the model. Also save the statisctics and the model for later use\n",
    "\n",
    "model = FaceIdentifier().to(device)\n",
    "train_losses, train_accuracies, val_losses, val_accuracies = train_loop(model, dataset_centered, dataset_validation, optimizer_name='RMSProp', num_epochs=25, batch_size=256, device=device, learning_rate=1e-5)\n",
    "\n",
    "th.save(model.state_dict(), 'saved_models/model.pth')\n",
    "th.save(train_losses, 'saved_losses/train_losses_no_regularization.pt')\n",
    "th.save(train_accuracies, 'saved_losses/train_accuracies_no_regularization.pt')\n",
    "th.save(val_losses, 'saved_losses/val_losses_no_regularization.pt')\n",
    "th.save(val_accuracies, 'saved_losses/val_accuracies_no_regularization.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRL8l9cDt0VP"
   },
   "outputs": [],
   "source": [
    "# # If you are working on Colab, downloads files automatically. Give colab permission to perform multiple file downloads.\n",
    "# from google.colab import files\n",
    "# files.download('/content/saved_losses/train_losses_no_regularization.pt')\n",
    "# files.download('/content/saved_losses/train_accuracies_no_regularization.pt')\n",
    "# files.download('/content/saved_losses/val_losses_no_regularization.pt')\n",
    "# files.download('/content/saved_losses/val_accuracies_no_regularization.pt')\n",
    "# files.download('/content/saved_models/model.pth') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fY1IapfDySdE",
    "outputId": "3e973109-31fc-4182-e9af-38411be47198"
   },
   "outputs": [],
   "source": [
    "# Modified VGG16 network. Last fully connected layer removed from the pretrained network. All other parameters are frozen.\n",
    "# An additional layer added to match 4000 classes of our dataset\n",
    "class ModifiedVGG16(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ModifiedVGG16,self).__init__()\n",
    "        self.net = models.vgg16(pretrained=True)\n",
    "        \n",
    "        for p in self.net.parameters(): # freeze parameters\n",
    "            p.requires_grad=False\n",
    "        \n",
    "        num_features = self.net.classifier[6].in_features\n",
    "        features = list(self.net.classifier.children())[:-1] # Remove last layer\n",
    "        self.net.classifier = nn.Sequential(*features) # Replace the model classifier\n",
    "        self.output_layer = nn.Linear(num_features, num_classes) # Add our layer with 4 outputs\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.net(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mA8eV7j2w02D"
   },
   "outputs": [],
   "source": [
    "vgg16_model = ModifiedVGG16(4000).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DvdQTBjyxScb"
   },
   "outputs": [],
   "source": [
    "train_losses, train_accuracies, val_losses, val_accuracies = train_loop(vgg16_model, dataset_centered, dataset_validation, optimizer_name='RMSProp', num_epochs=1, batch_size=256, device=device, learning_rate=1e-5)\n",
    "\n",
    "th.save(model.state_dict(), 'saved_models/model.pth')\n",
    "th.save(train_losses, 'saved_losses/vgg16_train_losses_no_regularization.pt')\n",
    "th.save(train_accuracies, 'saved_losses/vgg16_train_accuracies_no_regularization.pt')\n",
    "th.save(val_losses, 'saved_losses/vgg16_val_losses_no_regularization.pt')\n",
    "th.save(val_accuracies, 'saved_losses/vgg16_val_accuracies_no_regularization.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the models with the validate function.\n",
    "loss, accuracy = validate(vgg16_model, dataset_test, batch_size=256, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "jaber_hw.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
