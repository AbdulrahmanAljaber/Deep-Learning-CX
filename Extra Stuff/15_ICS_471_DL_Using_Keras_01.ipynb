{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "15-ICS_471_DL_Using_Keras_01.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlMSbx-ddk0M"
      },
      "source": [
        "# ICS 471-Deep Learning using Keras\n",
        "\n",
        "We will cover how to develop real-world deep learning models using Keras.\n",
        "\n",
        "First you need to have keras installed on your machine (if you're using colab, it is already installed)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzNuFjrDBgMm"
      },
      "source": [
        "\n",
        "\n",
        "## Data loading\n",
        "\n",
        "Keras models accept three types of inputs:\n",
        "\n",
        "- **NumPy arrays**, just like Scikit-Learn and many other Python-based libraries. This\n",
        " is a good option if your data fits in memory.\n",
        "- **[TensorFlow `Dataset` objects](https://www.tensorflow.org/guide/data)**. This is a\n",
        "high-performance option that is more suitable for datasets that do not fit in memory\n",
        " and that are streamed from disk or from a distributed filesystem.\n",
        "- **Python generators** that yield batches of data (such as custom subclasses of\n",
        "the `keras.utils.Sequence` class).\n",
        "\n",
        "Before you start training a model, you will need to make your data available as one of\n",
        "these formats. If you have a large dataset and you are training on GPU(s), consider\n",
        "using `Dataset` objects, since they will take care of performance-critical details,\n",
        " such as:\n",
        "\n",
        "- Asynchronously preprocessing your data on CPU while your GPU is busy, and buffering\n",
        " it into a queue.\n",
        "- Prefetching data on GPU memory so it's immediately available when the GPU has\n",
        " finished processing the previous batch, so you can reach full GPU utilization.\n",
        "\n",
        "Keras features a range of utilities to help you turn raw data on disk into a `Dataset`:\n",
        "\n",
        "- `tf.keras.preprocessing.image_dataset_from_directory` turns image files sorted into\n",
        " class-specific folders into a labeled dataset of image tensors.\n",
        "- `tf.keras.preprocessing.text_dataset_from_directory` does the same for text files.\n",
        "\n",
        "In addition, the TensorFlow `tf.data` includes other similar utilities, such as\n",
        "`tf.data.experimental.make_csv_dataset` to load structured data from CSV files.\n",
        "\n",
        "**Example: obtaining a labeled dataset from image files on disk**\n",
        "\n",
        "Supposed you have image files sorted by class in different folders, like this:\n",
        "\n",
        "```\n",
        "main_directory/\n",
        "...class_a/\n",
        "......a_image_1.jpg\n",
        "......a_image_2.jpg\n",
        "...class_b/\n",
        "......b_image_1.jpg\n",
        "......b_image_2.jpg\n",
        "```\n",
        "\n",
        "Then you can do:\n",
        "\n",
        "```python\n",
        "# Create a dataset.\n",
        "dataset = keras.preprocessing.image_dataset_from_directory(\n",
        "  'path/to/main_directory', batch_size=64, image_size=(200, 200))\n",
        "\n",
        "# For demonstration, iterate over the batches yielded by the dataset.\n",
        "for data, labels in dataset:\n",
        "   print(data.shape)  # (64, 200, 200, 3)\n",
        "   print(data.dtype)  # float32\n",
        "   print(labels.shape)  # (64,)\n",
        "   print(labels.dtype)  # int32\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "**Example: obtaining a labeled dataset from text files on disk**\n",
        "\n",
        "Likewise for text: if you have `.txt` documents sorted by class in different folders,\n",
        " you can do:\n",
        "\n",
        "```python\n",
        "dataset = keras.preprocessing.text_dataset_from_directory(\n",
        "  'path/to/main_directory', batch_size=64)\n",
        "\n",
        "# For demonstration, iterate over the batches yielded by the dataset.\n",
        "for data, labels in dataset:\n",
        "   print(data.shape)  # (64,)\n",
        "   print(data.dtype)  # string\n",
        "   print(labels.shape)  # (64,)\n",
        "   print(labels.dtype)  # int32\n",
        "```\n",
        "\n",
        "\n",
        "### Data available in Keras \n",
        "If the data is available within the datasets provided by keras, you can load it directly using load_data() as shown in today tutorial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjoCqlDeapbC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "410bb518-54ef-4095-ee63-5a661c7f9592"
      },
      "source": [
        "# load data\n",
        "from keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0NEi9Xwarpi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14ad4b16-be22-405f-b06d-35cd50f48044"
      },
      "source": [
        "# reshape to be [samples][width][height][channels]\n",
        "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1)).astype('float32')\n",
        "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1)).astype('float32')\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28, 1)\n",
            "(10000, 28, 28, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdAd1mG6DtIM"
      },
      "source": [
        "## Data preprocessing with Keras\n",
        "\n",
        "Once your data is in the form of string/int/float NumpPy arrays, or a `Dataset` object\n",
        " (or Python generator) that yields batches of string/int/float tensors,\n",
        "it is time to **preprocess** the data. This can mean:\n",
        "\n",
        "- Tokenization of string data, followed by token indexing.\n",
        "- Feature normalization.\n",
        "- Rescaling the data to small values (in general, input values to a neural network\n",
        "should be close to zero -- typically we expect either data with zero-mean and\n",
        " unit-variance, or data in the `[0, 1]` range.\n",
        "\n",
        "### How to do preprocessing in Keras?\n",
        "We can do one of the following:\n",
        "\n",
        "1.   Preprocess the data before fitting it to the model, or\n",
        "2.   Preprocess the data within the model (next lecture)\n",
        "\n",
        "In this lecture, we will preprocess the data before fitting it to the mode. This can be done in two different ways:\n",
        "\n",
        "1.   Do preprocessing manually\n",
        "2.   Use objects to do preprocessing \n",
        "\n",
        "\n",
        "**Example: Preprocessing the data manually and using objects**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33NcGd32NOwI",
        "outputId": "c2323ff8-a5c8-4b51-ae23-e087efc8a900"
      },
      "source": [
        "# Method 01: normalize inputs from 0-255 to 0-1\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "'''\n",
        "#### Method 02: Do preprocessing using objects: \n",
        "import numpy as np\n",
        "from keras.layers import Rescaling\n",
        "scaler = Rescaling(scale=1.0 / 255)\n",
        " \n",
        "X_train = scaler(X_train)\n",
        "X_test = scaler(X_test)\n",
        "''''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "var: 0.0949\n",
            "mean: 0.1307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbzdIQG6RWz0"
      },
      "source": [
        "## Building models with the Keras Functional API\n",
        "\n",
        "Now, we will build our model using Keras API. We need first to have a data for validation. To do this, we will split the train data (X_train) into train (70%) and validation (30%) data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7trnBCDTpTEt"
      },
      "source": [
        "#  split train data into train and val data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.30, random_state=4, stratify=y_train)\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8v02WIMV3bD"
      },
      "source": [
        "We need also to convert the labels to one hot encode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpArJpFla2Tp"
      },
      "source": [
        "# one hot encode outputs\n",
        "from keras.utils import np_utils\n",
        "\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_val = np_utils.to_categorical(y_val)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]\n",
        "print(\"Number of classes: \", num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_EP3UbEWZDo"
      },
      "source": [
        "### Build the model\n",
        "\n",
        "We can create a NN layer using the different layer types available in keras.layers package. For example, to create a fully connected layer with 16 neurons output.\n",
        "\n",
        "```python\n",
        "dense = keras.layers.Dense(units=16)\n",
        "```\n",
        "\n",
        "There are two ways to build models in Keras:\n",
        "\n",
        "1.   Sequential\n",
        "2.   Functional API\n",
        "\n",
        "The most common and most powerful way to build Keras models is the Functional API. In the following code, both methods will be covered. To\n",
        "build models, you start by specifying the shape (and\n",
        "optionally the dtype) of your inputs. If any dimension of your input can vary, you can\n",
        "specify it as `None`. For instance, an input for 200x200 RGB image would have shape\n",
        "`(200, 200, 3)`, but an input for RGB images of any size would have shape `(None,\n",
        " None, 3)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LEB69YYaFl-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "298a91d0-b6cd-4c90-eadf-cf9381aa24a6"
      },
      "source": [
        "# define the CNN model using Sequential method\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout,Flatten \n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "\n",
        "def build_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv2D(30, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
        "\tmodel.add(MaxPooling2D())\n",
        "\tmodel.add(Conv2D(15, (3, 3), activation='relu'))\n",
        "\tmodel.add(MaxPooling2D())\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(128, activation='relu'))\n",
        "\tmodel.add(Dense(50, activation='relu'))\n",
        "\tmodel.add(Dense(num_classes, activation='softmax'))\n",
        "\treturn model\n",
        "\n",
        "'''\n",
        "# define the CNN model using functional API method\n",
        "def build_model():\n",
        "\t# create model\n",
        "  input_data = keras.Input(shape=(28, 28, 3))\n",
        "\tx = Conv2D(30, (5, 5),  activation='relu')(input_data)\n",
        "\tx = MaxPooling2D()(x)\n",
        "\tx = Conv2D(15, (3, 3), activation='relu')(x)\n",
        "\tx = MaxPooling2D()(x)\n",
        "\tx = Dropout(0.2)(x)\n",
        "\tx = Flatten()(x)\n",
        "\tx = Dense(128, activation='relu')(x)\n",
        "\tx = Dense(50, activation='relu')(x)\n",
        "\toutput = Dense(num_classes, activation='softmax')(x)\n",
        " \tmodel = keras.Model(inputs=input_data, outputs=output)\n",
        "\treturn model\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# define the CNN model using functional API method\\ndef build_model():\\n\\t# create model\\n  input_data = keras.Input(shape=(28, 28, 3))\\n\\tx = Conv2D(30, (5, 5),  activation='relu')(input_data)\\n\\tx = MaxPooling2D()(x)\\n\\tx = Conv2D(15, (3, 3), activation='relu')(x)\\n\\tx = MaxPooling2D()(x)\\n\\tx = Dropout(0.2)(x)\\n\\tx = Flatten()(x)\\n\\tx = Dense(128, activation='relu')(x)\\n\\tx = Dense(50, activation='relu')(x)\\n\\toutput = Dense(num_classes, activation='softmax')(x)\\n \\tmodel = keras.Model(inputs=input_data, outputs=output)\\n\\treturn model\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4eDcsKFZoYQ"
      },
      "source": [
        "Now, we will do the following:\n",
        "\n",
        "\n",
        "*   Build the model\n",
        "*   Print the model summary\n",
        "*   Compile the model to define optimizer, loss function, and evaluation metric(s).\n",
        "*   Start training the model by fitting it with the data \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIDagb7qbHtF"
      },
      "source": [
        "# build the model\n",
        "model = build_model()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJ8O2CYoXkf8",
        "outputId": "476703a2-1d57-4924-9da1-be0e7b19efe7"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 24, 24, 30)        780       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 12, 12, 30)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 10, 10, 15)        4065      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 5, 5, 15)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 5, 5, 15)          0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 375)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               48128     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 50)                6450      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                510       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 59,933\n",
            "Trainable params: 59,933\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk0ZfIDia17w"
      },
      "source": [
        "### Training models with `fit()`\n",
        "\n",
        "At this point, you know:\n",
        "\n",
        "- How to prepare your data (e.g. as a NumPy array or a `tf.data.Dataset` object)\n",
        "- How to build a model that will process your data\n",
        "\n",
        "The next step is to train your model on your data. The `Model` class features a\n",
        "built-in training loop, the `fit()` method. It accepts `Dataset` objects, Python\n",
        " generators that yield batches of data, or NumPy arrays.\n",
        "\n",
        "Before you can call `fit()`, you need to specify an optimizer and a loss function . This is the `compile()` step:\n",
        "\n",
        "```python\n",
        "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
        "              loss=keras.losses.CategoricalCrossentropy())\n",
        "```\n",
        "\n",
        "Loss and optimizer can be specified via their string identifiers (in this case\n",
        "their default constructor argument values are used):\n",
        "\n",
        "\n",
        "```python\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "```\n",
        "\n",
        "Once your model is compiled, you can start \"fitting\" the model to the data.\n",
        "Here's what fitting a model looks like with NumPy data:\n",
        "\n",
        "```python\n",
        "model.fit(numpy_array_of_samples, numpy_array_of_labels,validation_data=(X_val, y_val),\n",
        "          batch_size=32, epochs=10)\n",
        "```\n",
        "\n",
        "Besides the data, you have to specify two key parameters: the `batch_size` and\n",
        "the number of epochs (iterations on the data). Here our data will get sliced on batches\n",
        " of 32 samples, and the model will iterate 10 times over the data during training.\n",
        "\n",
        "Here's what fitting a model looks like with a dataset:\n",
        "\n",
        "```python\n",
        "model.fit(dataset_of_samples_and_labels, epochs=10)\n",
        "```\n",
        "\n",
        "Since the data yielded by a dataset is expected to be already batched, you don't need to\n",
        " specify the batch size here.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9Q_n_LVa1YZ"
      },
      "source": [
        "# Compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sodas6TTbKEE",
        "outputId": "60df3333-8059-433f-c039-de3726a0e180"
      },
      "source": [
        "# Fit the model\n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=200)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "210/210 [==============================] - 28s 131ms/step - loss: 0.1772 - accuracy: 0.9470 - val_loss: 0.0982 - val_accuracy: 0.9702\n",
            "Epoch 2/10\n",
            "210/210 [==============================] - 27s 130ms/step - loss: 0.1239 - accuracy: 0.9609 - val_loss: 0.0850 - val_accuracy: 0.9733\n",
            "Epoch 3/10\n",
            "210/210 [==============================] - 27s 129ms/step - loss: 0.0995 - accuracy: 0.9696 - val_loss: 0.0752 - val_accuracy: 0.9769\n",
            "Epoch 4/10\n",
            "210/210 [==============================] - 27s 129ms/step - loss: 0.0827 - accuracy: 0.9737 - val_loss: 0.0680 - val_accuracy: 0.9805\n",
            "Epoch 5/10\n",
            "210/210 [==============================] - 27s 130ms/step - loss: 0.0715 - accuracy: 0.9777 - val_loss: 0.0602 - val_accuracy: 0.9811\n",
            "Epoch 6/10\n",
            "210/210 [==============================] - 27s 130ms/step - loss: 0.0630 - accuracy: 0.9794 - val_loss: 0.0533 - val_accuracy: 0.9837\n",
            "Epoch 7/10\n",
            "210/210 [==============================] - 27s 130ms/step - loss: 0.0527 - accuracy: 0.9824 - val_loss: 0.0589 - val_accuracy: 0.9829\n",
            "Epoch 8/10\n",
            "210/210 [==============================] - 27s 129ms/step - loss: 0.0544 - accuracy: 0.9821 - val_loss: 0.0535 - val_accuracy: 0.9857\n",
            "Epoch 9/10\n",
            "210/210 [==============================] - 27s 129ms/step - loss: 0.0467 - accuracy: 0.9851 - val_loss: 0.0539 - val_accuracy: 0.9851\n",
            "Epoch 10/10\n",
            "210/210 [==============================] - 27s 128ms/step - loss: 0.0445 - accuracy: 0.9854 - val_loss: 0.0502 - val_accuracy: 0.9869\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_UH_RDLcKGz"
      },
      "source": [
        "The `fit()` call returns a \"history\" object which records what happened over the course\n",
        "of training. The `history.history` dict contains per-epoch timeseries of metrics\n",
        "values (here we have only one metric, the loss, and one epoch, so we only get a single\n",
        " scalar):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG63eyoPcNfo",
        "outputId": "d68d17c3-c340-4273-b823-288b7f37aed2"
      },
      "source": [
        "print(history.history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': [0.1771569550037384, 0.123890720307827, 0.09951203316450119, 0.08267467468976974, 0.07153205573558807, 0.06300043314695358, 0.052727583795785904, 0.05444664508104324, 0.04670879617333412, 0.04445866867899895], 'accuracy': [0.9470238089561462, 0.9608571529388428, 0.9696190357208252, 0.973714292049408, 0.9777143001556396, 0.9794047474861145, 0.9823571443557739, 0.9821428656578064, 0.9850714206695557, 0.9854047894477844], 'val_loss': [0.09816756099462509, 0.08499215543270111, 0.0752040296792984, 0.06801052391529083, 0.060166917741298676, 0.05328227952122688, 0.0588645301759243, 0.05353664606809616, 0.05393022671341896, 0.050183359533548355], 'val_accuracy': [0.9702222347259521, 0.9732778072357178, 0.9769444465637207, 0.9804999828338623, 0.9811111092567444, 0.9837222099304199, 0.9828888773918152, 0.9857222437858582, 0.9850555658340454, 0.9868888854980469]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMR1rDcHc9-X"
      },
      "source": [
        "### Using callbacks for checkpointing (and more)\n",
        "\n",
        "If training goes on for more than a few minutes, it's important to save your model at\n",
        " regular intervals during training. You can then use your saved models\n",
        "to restart training in case your training process crashes (this is important for\n",
        "multi-worker distributed training, since with many workers at least one of them is\n",
        " bound to fail at some point).\n",
        "\n",
        "An important feature of Keras is **callbacks**, configured in `fit()`. Callbacks are\n",
        " objects that get called by the model at different point during training, in particular:\n",
        "\n",
        "- At the beginning and end of each batch\n",
        "- At the beginning and end of each epoch\n",
        "\n",
        "Callbacks are a way to make model trainable entirely scriptable.\n",
        "\n",
        "You can use callbacks to periodically save your model. Here's a simple example: a\n",
        " `ModelCheckpoint` callback\n",
        "configured to save the model at the end of every epoch. The filename will include the\n",
        " current epoch.\n",
        "\n",
        "```python\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath='path/to/my/model_{epoch}',\n",
        "        save_freq='epoch')\n",
        "]\n",
        "model.fit(dataset, epochs=2, callbacks=callbacks)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QTftqZ2dAT2"
      },
      "source": [
        "### Monitoring training progress with TensorBoard\n",
        "\n",
        "Staring at the Keras progress bar isn't the most ergonomic way to monitor how your loss\n",
        " and metrics are evolving over time. There's a better solution:\n",
        "[TensorBoard](https://www.tensorflow.org/tensorboard),\n",
        "a web application that can display real-time graphs of your metrics (and more).\n",
        "\n",
        "To use TensorBoard with `fit()`, simply pass a `keras.callbacks.TensorBoard` callback\n",
        " specifying the directory where to store TensorBoard logs:\n",
        "\n",
        "\n",
        "```python\n",
        "callbacks = [\n",
        "    keras.callbacks.TensorBoard(log_dir='./logs')\n",
        "]\n",
        "model.fit(dataset, epochs=2, callbacks=callbacks)\n",
        "```\n",
        "\n",
        "You can then launch a TensorBoard instance that you can open in your browser to monitor\n",
        " the logs getting written to this location:\n",
        "\n",
        "```\n",
        "tensorboard --logdir=./logs\n",
        "```\n",
        "\n",
        "What's more, you can launch an in-line TensorBoard tab when training models in Jupyter\n",
        " / Colab notebooks.\n",
        "[Here's more information](https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnAd_UThdM8d"
      },
      "source": [
        "### Evaluate the model on the test data\n",
        "After we trained the model, we will evaluate it on the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rr1x9r-nbOuc",
        "outputId": "12f0807c-da3a-4e39-c88f-0dcfd6ab7f45"
      },
      "source": [
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"CNN Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN Accuracy: 98.58%\n"
          ]
        }
      ]
    }
  ]
}